{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, tqdm, glob, os.path as osp\n",
    "import h5py, numpy as np, pandas as pd, numba\n",
    "import torch, torch_geometric as tg, multiprocessing as mp\n",
    "from uuid import uuid4\n",
    "if '/scratch' not in sys.path: sys.path.append('/scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graph from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timer\n",
    "def get_graph(f, idx):\n",
    "    \n",
    "    # The event table contains one entry per event\n",
    "    evt = f['event_table']\n",
    "    if idx >= evt['event'].shape[0]: raise Exception(f'Graph {idx} larger than file size {evt.shape[0]}')\n",
    "    graph = f['graph_table']\n",
    "\n",
    "    # Get event information\n",
    "    run = evt['run'][idx].squeeze()\n",
    "    subrun = evt['subrun'][idx].squeeze()\n",
    "    event = evt['event'][idx].squeeze()\n",
    "\n",
    "    # Pull out all graph nodes associated with that event\n",
    "    cut = (graph['run'][:,0] == run) & (graph['subrun'][:,0] == subrun) & (graph['event'][:,0] == event)\n",
    "\n",
    "    return pd.DataFrame(np.array([graph[key][cut,0] for key in graph.keys()]).T, columns=list(graph.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timer\n",
    "def get_particle_tree(f, idx):\n",
    "    \n",
    "    # The event table contains one entry per event\n",
    "    evt = f['event_table']\n",
    "    if idx >= evt['event'].shape[0]: raise Exception(f'Graph {idx} larger than file size {evt.shape[0]}')\n",
    "    part = f['particle_table']\n",
    "\n",
    "    # Get event information\n",
    "    run = evt['run'][idx].squeeze()\n",
    "    subrun = evt['subrun'][idx].squeeze()\n",
    "    event = evt['event'][idx].squeeze()\n",
    "\n",
    "    # Pull out all graph nodes associated with that event\n",
    "    cut = (part['run'][:,0] == run) & (part['subrun'][:,0] == subrun) & (part['event'][:,0] == event)\n",
    "\n",
    "    return pd.DataFrame(np.array([part[key][cut,0] for key in part.keys()]).T, columns=list(part.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def _create_edges(wire, time):\n",
    "    edges = []\n",
    "    for start, (ws, ts) in enumerate(zip(wire, time)):\n",
    "        for end, (we, te) in enumerate(zip(wire, time)):\n",
    "            if start == end: continue # no self loops\n",
    "            elif ws >= we or we-ws > 5: continue # within 5 wires\n",
    "            elif abs(te-ts) > 50: continue # within 50 time ticks\n",
    "            edges.append((start, end))\n",
    "    return np.array(edges).T\n",
    "\n",
    "def create_edges(df):\n",
    "    wire = df.wire.to_list()\n",
    "    time = df.time.to_list()\n",
    "    return _create_edges(wire, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate dataframes and get truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timer\n",
    "def get_primaries(df):\n",
    "    \n",
    "    # Get the primary particle ID for each particle in the hierarchy\n",
    "    primaries = []\n",
    "    df_dict = df.to_dict(orient='list')\n",
    "    parent_dict = { key: val for key, val in zip(df_dict['id'], df_dict['parent_id'])}\n",
    "    \n",
    "    for id, parent in parent_dict.items():\n",
    "        tmp_id = id\n",
    "        while True:\n",
    "            # If we've found a primary then quit\n",
    "            if parent_dict[tmp_id] == 0: break\n",
    "            # Otherwise walk back a step along the particle hierarchy\n",
    "            tmp_id = parent_dict[tmp_id]\n",
    "        primaries.append(tmp_id)\n",
    "    df['primary'] = primaries\n",
    "    \n",
    "    # Go from primary ID to primary type\n",
    "    primtypes = { row.id: row.type for _, row in df[(df.parent_id == 0)].iterrows() }\n",
    "    types = [ primtypes[primary] for primary in primaries ]\n",
    "    \n",
    "    for i in range(len(types)):\n",
    "        if abs(types[i]) == 11: types[i] = 1\n",
    "#             if df['parent_id'][i] != 0: types[i] = 1 # EM shower\n",
    "#             else: types[i] = 2   # EM shower root\n",
    "        elif abs(types[i]) == 13: types[i] = 2 # muon track\n",
    "        else: types[i] = 3 # Everything else\n",
    "            \n",
    "    df['truth'] = types\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.jit\n",
    "def _add_to_graph(true_id, truth, primary, graph_id):\n",
    "    '''Take truth information from particle-wise dataframe, and add to hit-wise dataframe'''\n",
    "    truth_dict = { key: val for key, val in zip(true_id, truth) }\n",
    "    primary_dict = { key: val for key, val in zip(true_id, primary) }\n",
    "    graph_truth = [ truth_dict[id] for id in graph_id ]\n",
    "    graph_primary = [ primary_dict[id] for id in graph_id ]\n",
    "    return graph_truth, graph_primary\n",
    "\n",
    "# @timer\n",
    "def add_to_graph(df_graph, df_part):\n",
    "    '''Take truth information from particle-wise dataframe, and add to hit-wise dataframe'''\n",
    "    true_id = df_part.id.to_list()\n",
    "    truth = df_part.truth.to_list()\n",
    "    primary = df_part.primary.to_list()\n",
    "    graph_id = df_graph.true_id.to_list()\n",
    "    graph_truth, graph_primary = _add_to_graph(true_id, truth, primary, graph_id)\n",
    "    df_graph['truth'] = graph_truth\n",
    "    df_graph['primary'] = graph_primary\n",
    "    return df_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timer\n",
    "@numba.jit\n",
    "def get_truth(graph, part, edges):\n",
    "    \n",
    "    hit_truth = graph.truth.to_list()\n",
    "    hit_true_id = graph.true_id.to_list()\n",
    "    hit_primary = graph.primary.to_list()\n",
    "    \n",
    "    truth = []\n",
    "    \n",
    "    for e_in, e_out in edges.T:\n",
    "        \n",
    "        if hit_truth[e_in] == 1: # if em shower\n",
    "            if hit_primary[e_in] == hit_primary[e_out]: truth.append(hit_truth[e_in])\n",
    "            else: truth.append(0)\n",
    "        else:\n",
    "            if hit_true_id[e_in] == hit_true_id[e_out]: truth.append(hit_truth[e_in])\n",
    "            else: truth.append(0)\n",
    "\n",
    "    return np.array(truth).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a single file\n",
    "This function loops over a single HDF5 file and processes it into input PyTorch files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    from timeit import default_timer\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        start = default_timer()\n",
    "        value = func(*args, **kwargs)\n",
    "        print('executing', func.__name__, 'took', default_timer()-start, 'seconds')\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timer\n",
    "def process(file):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for idx in range(f['event_table']['event'].shape[0]):\n",
    "            graph = get_graph(f, idx)\n",
    "            part = get_particle_tree(f, idx)\n",
    "            part = get_primaries(part)\n",
    "            graph = add_to_graph(graph, part)\n",
    "            \n",
    "            for plane in range(3):\n",
    "                graph_plane = graph[(graph.plane==plane)].reset_index(drop=True)\n",
    "                if graph_plane.shape[0] < 50: continue\n",
    "#                 tmp = tg.data.Data(pos=torch.FloatTensor(graph_plane.loc[:, ['wire', 'time']].values))\n",
    "#                 tmp = tg.transforms.Delaunay()(tmp)\n",
    "#                 edges = tg.transforms.FaceToEdge()(tmp).edge_index\n",
    "                edges = create_edges(graph_plane)\n",
    "                truth = get_truth(graph_plane, part, edges)\n",
    "\n",
    "                x = graph_plane.loc[:, ['plane', 'wire', 'time',\n",
    "                                         'tpc', 'rawplane', 'rawwire', 'rawtime', \n",
    "                                         'integral', 'rms' ]].values\n",
    "\n",
    "                data = {'x': x, 'edge_index': edges, 'y': truth}\n",
    "                data = tg.data.Data(**data)\n",
    "\n",
    "                torch.save(data, f'/data/hit2d/processed-flav-shower/{uuid4()}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dataset\n",
    "\n",
    "We get a list of all the H5 files, and then map them to a pool of processes to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-81-d45084540014>\", line 8, in process\n    graph = add_to_graph(graph, part)\n  File \"<ipython-input-77-d546d2d0a152>\", line 17, in add_to_graph\n    graph_truth, graph_primary = _add_to_graph(true_id, truth, primary, graph_id)\n  File \"<ipython-input-77-d546d2d0a152>\", line 6, in _add_to_graph\n    graph_truth = [ truth_dict[id] for id in graph_id ]\n  File \"<ipython-input-77-d546d2d0a152>\", line 6, in <listcomp>\n    graph_truth = [ truth_dict[id] for id in graph_id ]\nKeyError: 0.0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a4e17bb7a657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonswap\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfluxswap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# process(fluxswap[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0.0"
     ]
    }
   ],
   "source": [
    "nonswap = glob.glob('/data/hit2d/nonswap/*.h5')\n",
    "fluxswap = glob.glob('/data/hit2d/fluxswap/*.h5')\n",
    "files = nonswap + fluxswap\n",
    "\n",
    "with mp.Pool(processes=50) as pool: pool.map(process, files)\n",
    "# process(fluxswap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/data/hit2d/processed-flav-shower/*.pt': No such file or directory\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "!ls /data/hit2d/processed-flav-shower/*.pt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace\n",
    "What's in the HDF5 file? Is there a true particle table, and if so, how is it structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  13]\n",
      " [2112]\n",
      " [2212]\n",
      " ...\n",
      " [  11]\n",
      " [  11]\n",
      " [  11]]\n"
     ]
    }
   ],
   "source": [
    "nonswap = glob.glob('/data/hit2d/nonswap/*.h5')\n",
    "name = nonswap[0]\n",
    "with h5py.File(name, 'r') as file:\n",
    "    print(file['particle_table']['type'][()])\n",
    "#     print(file['particle_table']['trie_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mc\n",
    "\n",
    "for j, file in enumerate(glob.glob('/data/hit2d/processed-flav-shower-delaunay/*.pt')):\n",
    "    \n",
    "    \n",
    "    data = torch.load(file)\n",
    "    wire = data.x[:,1]\n",
    "    time = data.x[:,2]\n",
    "    tpc = data.x[:,3]\n",
    "    \n",
    "    lines = [ [ [ wire[edge[0]], time[edge[0]] ], [ wire[edge[1]], time[edge[1]] ] ] for edge in data.edge_index.T ]\n",
    "\n",
    "    # Edge plot\n",
    "    lines_class = [ [], [], [], [], [] ]\n",
    "    colours = ['gainsboro', 'red', 'green', 'blue', 'yellow' ]\n",
    "    for l, y in zip(lines, data.y):\n",
    "        lines_class[y].append(l)\n",
    "    lcs = []\n",
    "    for i in range(5): lcs.append(mc.LineCollection(lines_class[i], colors=colours[i], linewidths=2, zorder=1))\n",
    "    fig, ax = plt.subplots(figsize=[16,9])\n",
    "    for lc in lcs: ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/flav-shower-delaunay/evt{j+1}_edges.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /data/hit2d/processed-flav/*.pt\n",
    "!rm plots/flav/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/hit2d/processed-flav | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /data/hit2d/processed-flav-debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in glob.glob('/data/hit2d/processed-flav-showers/*.pt'):\n",
    "    data = torch.load(name)\n",
    "    if len(data.y.shape) > 1:\n",
    "        print('shape of y is', data.y.shape)\n",
    "        os.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
