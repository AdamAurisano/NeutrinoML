{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First we need to correctly set the python environment. This is done by adding the top directory of the repository to the Python path. Once that's done, we can import various packages from inside the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os.path as osp, yaml, argparse, logging, math, numpy as np, torch, torchvision, sherpa, logging\n",
    "sys.path.append('/scratch') # This line is equivalent to doing source scripts/source_me.sh in a bash terminal\n",
    "from torch.utils.data import DataLoader\n",
    "import MinkowskiEngine as ME\n",
    "from Core.trainers import Trainer\n",
    "from glob import glob\n",
    "from Core import utils\n",
    "from SparseNOvA import datasets\n",
    "from Core import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring\n",
    "\n",
    "Most of the training options are set in a configuration YAML file. We're going to load this config, and then the options inside will be passed to the relevent piece of the training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/SparseNOvA/config/sparse_nova_mobilenet.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading inputs\n",
    "\n",
    "Here we load the dataset and the trainer, which is responsible for building the model and overseeing training. There's a block of code which is responsible for slicing the full dataset up into a training dataset and a validation dataset where jitter is applied to training dataset only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nus = sorted(glob(f'{config[\"data\"][\"filedir\"]}/nu/*.pt'))\n",
    "all_cosmics = sorted(glob(f'{config[\"data\"][\"filedir\"]}/cosmic/*.pt'))\n",
    "\n",
    "if len(all_cosmics) > int(0.1 * len(all_nus)):\n",
    "    all_cosmics = all_cosmics[0:int(0.1*len(all_nus))]\n",
    "\n",
    "fulllen_nu = len(all_nus)\n",
    "fulllen_cosmic = len(all_cosmics)\n",
    "\n",
    "tv_num_nu = math.ceil(fulllen_nu*config['data']['t_v_split'])\n",
    "tv_num_cosmic = math.ceil(fulllen_cosmic*config['data']['t_v_split'])\n",
    "\n",
    "splits_nu = np.cumsum([fulllen_nu - tv_num_nu, 0, tv_num_nu])\n",
    "splits_cos = np.cumsum([fulllen_cosmic - tv_num_cosmic, 0, tv_num_cosmic])\n",
    "# print(splits_nu)\n",
    "# print(splits_cos)\n",
    "\n",
    "train_files = all_nus[0:splits_nu[1]] + all_cosmics[0:splits_cos[1]]\n",
    "train_files.sort(key = lambda x: osp.basename(x))  \n",
    "train_dataset = datasets.get_dataset(filelist=train_files, apply_jitter=True, **config['data'])\n",
    "\n",
    "valid_files = all_nus[splits_nu[1]:splits_nu[2]] + all_cosmics[splits_cos[1]:splits_cos[2]]\n",
    "valid_files.sort(key = lambda x: osp.basename(x))\n",
    "valid_dataset = datasets.get_dataset(filelist=valid_files, apply_jitter=False, **config['data'])\n",
    "\n",
    "# parameters = [sherpa.Continuous('learning_rate', [1e-5, 1e-1]), sherpa.Continuous('weight_decay', [0.01, 0.1]), sherpa.Discrete('unet_depth', [2, 6])]\n",
    "\n",
    "trainer = Trainer(**config['trainer'])\n",
    "\n",
    "# alg = sherpa.algorithms.GPyOpt(max_num_trials=50)\n",
    "# study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True, dashboard_port=8000)\n",
    "\n",
    "collate = getattr(utils, config['model']['collate'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate, **config['data_loader'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, collate_fn=collate, **config['data_loader'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "The trainer will load the network architecture and compile it into a model.\n",
    "The second line will keep a log of the trainer for reference. \n",
    "The third line will use specific parameters which were successful in a previous training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.build_model(**config['model'])\n",
    "trainer.logger.addHandler(logging.FileHandler('/scratch/SparseNOvA/logs/train.log'))\n",
    "# trainer.load_state_dict(state_dict='params/checkpoints/model_checkpoint_Minkowski2StackClass_213908357_2e641e404d_haejunoh_005.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "Once all the setup is done, all that's left is to run training and save some summary statistics to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary = trainer.train(train_loader, valid_data_loader=valid_loader, **config['trainer'])\n",
    "print(train_summary)\n",
    "torch.save(train_summary, 'summary_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
