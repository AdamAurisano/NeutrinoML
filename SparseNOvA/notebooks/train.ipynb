{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First we need to correctly set the python environment. This is done by adding the top directory of the repository to the Python path. Once that's done, we can import various packages from inside the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, yaml, math, numpy as np, torch\n",
    "sys.path.append('/scratch') # This line is equivalent to doing source scripts/source_me.sh in a bash terminal\n",
    "from torch.utils.data import DataLoader\n",
    "from SparseBase.trainers import SparseTrainer\n",
    "from SparseBase import utils\n",
    "from SparseNOvA import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring\n",
    "\n",
    "Most of the training options are set in a configuration YAML file. We're going to load this config, and then the options inside will be passed to the relevent piece of the training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/SparseNOvA/config/sparse_nova.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading inputs\n",
    "\n",
    "Here we load the dataset and the trainer, which is responsible for building the model and overseeing training. There's a block of code which is responsible for slicing the full dataset up into a training dataset and a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.get_dataset(**config['data'])\n",
    "trainer = SparseTrainer(**config['trainer'])\n",
    "\n",
    "fulllen = len(full_dataset)\n",
    "tv_num = math.ceil(fulllen*config['data']['t_v_split'])\n",
    "splits = np.cumsum([fulllen-tv_num,0,tv_num])\n",
    "collate = utils.collate_sparse_minkowski_2stack\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=0,stop=splits[0]))\n",
    "valid_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=splits[1],stop=splits[2]))\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate, **config['data_loader'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, collate_fn=collate, **config['data_loader'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "The trainer will load the network architecture and compile it into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.build_model(**config['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "Once all the setup is done, all that's left is to run training and save some summary statistics to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary = trainer.train(train_loader, valid_data_loader=valid_loader, **config['trainer'])\n",
    "print(train_summary)\n",
    "torch.save(train_summary, 'summary_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
